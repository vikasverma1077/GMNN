/home/vermavik/virtualenv/al/lib/python3.6/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
mix_alpha0.5
mix_consis0.0
epoch :   0,loss:0.6354660392,loss_usup:0.6359823346, train_acc:0.633, dev_acc:0.508, test_acc:0.444, test_acc_ema:0.444
epoch : 400,loss:0.3051341474,loss_usup:0.4800239205, train_acc:0.950, dev_acc:0.750, test_acc:0.719, test_acc_ema:0.719
epoch : 800,loss:0.3237132728,loss_usup:0.5301390886, train_acc:0.950, dev_acc:0.788, test_acc:0.751, test_acc_ema:0.751
epoch :1200,loss:0.3652694225,loss_usup:0.5105946660, train_acc:0.967, dev_acc:0.734, test_acc:0.717
epoch :1600,loss:0.3472959399,loss_usup:0.4597366750, train_acc:0.983, dev_acc:0.764, test_acc:0.743
input dropout: 0.5, dropout: 0.5, lr: 0.01, tau: 0.1, alpha: 0.5, consistency: 0.0
Test acc 76.900

/home/vermavik/virtualenv/al/lib/python3.6/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
mix_alpha0.5
mix_consis0.0
epoch :   0,loss:0.6319641471,loss_usup:0.6360204220, train_acc:0.700, dev_acc:0.562, test_acc:0.586, test_acc_ema:0.586
epoch : 400,loss:0.3944087923,loss_usup:0.4399223328, train_acc:0.933, dev_acc:0.800, test_acc:0.758, test_acc_ema:0.758
epoch : 800,loss:0.3110613227,loss_usup:0.4226855040, train_acc:0.917, dev_acc:0.784, test_acc:0.745, test_acc_ema:0.745
epoch :1200,loss:0.3618737161,loss_usup:0.4758966565, train_acc:0.967, dev_acc:0.780, test_acc:0.745
epoch :1600,loss:0.4038424492,loss_usup:0.4929869473, train_acc:0.950, dev_acc:0.750, test_acc:0.726, test_acc_ema:0.726
input dropout: 0.5, dropout: 0.5, lr: 0.01, tau: 0.1, alpha: 0.5, consistency: 0.0
Test acc 77.100

/home/vermavik/virtualenv/al/lib/python3.6/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
mix_alpha0.5
mix_consis0.0
epoch :   0,loss:0.6350818872,loss_usup:0.6361812353, train_acc:0.467, dev_acc:0.428, test_acc:0.444, test_acc_ema:0.444
epoch : 400,loss:0.4062317014,loss_usup:0.4904254079, train_acc:0.950, dev_acc:0.788, test_acc:0.764
epoch : 800,loss:0.2701020241,loss_usup:0.4224751294, train_acc:0.950, dev_acc:0.780, test_acc:0.750, test_acc_ema:0.750
epoch :1200,loss:0.2037978023,loss_usup:0.5186336040, train_acc:0.950, dev_acc:0.790, test_acc:0.751, test_acc_ema:0.751
epoch :1600,loss:0.3713053763,loss_usup:0.5724912882, train_acc:0.983, dev_acc:0.758, test_acc:0.735
input dropout: 0.5, dropout: 0.5, lr: 0.01, tau: 0.1, alpha: 0.5, consistency: 0.0
Test acc 77.500

/home/vermavik/virtualenv/al/lib/python3.6/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
mix_alpha0.5
mix_consis0.0
epoch :   0,loss:0.6391060948,loss_usup:0.6358075738, train_acc:0.600, dev_acc:0.438, test_acc:0.442, test_acc_ema:0.442
epoch : 400,loss:0.4132016003,loss_usup:0.4543453753, train_acc:0.967, dev_acc:0.776, test_acc:0.745
epoch : 800,loss:0.3588777184,loss_usup:0.5214153528, train_acc:0.967, dev_acc:0.768, test_acc:0.749
epoch :1200,loss:0.1950707585,loss_usup:0.3885317445, train_acc:0.950, dev_acc:0.786, test_acc:0.749, test_acc_ema:0.749
epoch :1600,loss:0.1992414296,loss_usup:0.5200667381, train_acc:0.950, dev_acc:0.788, test_acc:0.762, test_acc_ema:0.762
input dropout: 0.5, dropout: 0.5, lr: 0.01, tau: 0.1, alpha: 0.5, consistency: 0.0
Test acc 76.800

